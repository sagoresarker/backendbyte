[{"content":" Introduction to OpenTelemetry OpenTelemetry is an open-source observability framework designed to create and manage telemetry data such as traces, metrics, and logs. It provides a collection of tools, APIs, and SDKs used to instrument, generate, collect, and export telemetry data for analysis. The project aims to standardize the way telemetry data is collected and transmitted, making it easier for organizations to gain insights into their software\u0026rsquo;s performance and behavior across different environments and technologies.\nCore Components of OpenTelemetry API The OpenTelemetry API defines the core interfaces for instrumentation. It provides language-specific implementations for creating spans, recording metrics, and generating logs.\nSDK The SDK implements the OpenTelemetry API and provides additional functionality like sampling, batching, and exporting.\nInstrumentation Libraries These language-specific libraries implement the OpenTelemetry API to automatically instrument common libraries and frameworks.\nCollector The OpenTelemetry Collector is a vendor-agnostic proxy that can receive, process, and export telemetry data.\nExporters Exporters are responsible for sending the collected telemetry data to various backends (e.g., Jaeger, Prometheus, Zipkin) for storage and analysis.\nPropagators Propagators serialize and deserialize context data for distributed tracing across service boundaries.\nContext OpenTelemetry uses context to store and propagate telemetry data across different components and services.\nConfiguration Concepts Instrumentation OpenTelemetry supports both manual and automatic instrumentation:\nManual Instrumentation: Developers add OpenTelemetry API calls directly in their code. Automatic Instrumentation: Instrumentation libraries automatically capture telemetry from common frameworks and libraries. Sampling OpenTelemetry supports various sampling strategies:\nHead-based: Decisions on whether to sample a trace are made at the beginning of the request. Tail-based: Sampling decisions are made after the trace is complete. Adaptive: Sampling rates are adjusted dynamically based on observed data patterns and system load. Resource Detection The SDK can be configured to automatically detect and attach resource information to telemetry data.\nContext Propagation OpenTelemetry defines how context is propagated across service boundaries.\nBatching and Exporting Configuration options control how often telemetry data is batched and sent to backends.\nCollector Configuration The OpenTelemetry Collector is highly configurable with receivers, processors, and exporters.\nSecure Communication OpenTelemetry supports configuring TLS/SSL for secure communication between components and with backend systems.\nImplementing OpenTelemetry in Go Setting Up a Tracer import ( \u0026#34;go.opentelemetry.io/otel\u0026#34; \u0026#34;go.opentelemetry.io/otel/sdk/trace\u0026#34; ) func initTracer() (*trace.TracerProvider, error) { exporter, err := otlptrace.New(ctx, otlptracegrpc.NewClient()) if err != nil { return nil, err } tp := trace.NewTracerProvider( trace.WithBatcher(exporter), trace.WithResource(resource.NewWithAttributes( semconv.SchemaURL, semconv.ServiceNameKey.String(\u0026#34;my-service\u0026#34;), )), ) otel.SetTracerProvider(tp) return tp, nil } Creating a Span import \u0026#34;go.opentelemetry.io/otel/trace\u0026#34; func performOperation(ctx context.Context) { tr := otel.Tracer(\u0026#34;my-service\u0026#34;) ctx, span := tr.Start(ctx, \u0026#34;operation-name\u0026#34;) defer span.End() // Perform the operation // ... // Add attributes to the span span.SetAttributes(attribute.String(\u0026#34;key\u0026#34;, \u0026#34;value\u0026#34;)) } Span Naming Conventions Use descriptive, hierarchical names (e.g., \u0026ldquo;http.request\u0026rdquo;, \u0026ldquo;database.query\u0026rdquo;) Include the operation being performed (e.g., \u0026ldquo;user.create\u0026rdquo;, \u0026ldquo;payment.process\u0026rdquo;) Be consistent across services for similar operations Avoid including dynamic data in span names (use attributes instead) Advanced Features and Best Practices Metrics import \u0026#34;go.opentelemetry.io/otel/metric/instrument\u0026#34; meter := otel.Meter(\u0026#34;my-service\u0026#34;) counter, _ := meter.Int64Counter(\u0026#34;my.counter\u0026#34;) counter.Add(ctx, 1, attribute.String(\u0026#34;key\u0026#34;, \u0026#34;value\u0026#34;)) Sampling Strategies sampler := trace.ParentBased(trace.TraceIDRatioBased(0.1)) tp := trace.NewTracerProvider( trace.WithSampler(sampler), // ... other options ) Resource Detection import ( \u0026#34;go.opentelemetry.io/otel/sdk/resource\u0026#34; \u0026#34;go.opentelemetry.io/otel/semconv/v1.4.0\u0026#34; ) res, _ := resource.New(ctx, resource.WithFromEnv(), resource.WithProcess(), resource.WithTelemetrySDK(), resource.WithHost(), resource.WithAttributes( semconv.ServiceNameKey.String(\u0026#34;my-service\u0026#34;), semconv.ServiceVersionKey.String(\u0026#34;v1.0.0\u0026#34;), ), ) Error Handling and Status OpenTelemetry provides a way to record errors and set the status of a span. This is crucial for understanding the outcome of operations and quickly identifying issues in your distributed system.\nRecording Errors When an error occurs, you can record it on the span:\nimport ( \u0026#34;go.opentelemetry.io/otel/codes\u0026#34; \u0026#34;go.opentelemetry.io/otel/trace\u0026#34; ) func performOperation(ctx context.Context) error { tr := otel.Tracer(\u0026#34;my-service\u0026#34;) ctx, span := tr.Start(ctx, \u0026#34;operation-name\u0026#34;) defer span.End() err := someFunction() if err != nil { span.RecordError(err) span.SetStatus(codes.Error, err.Error()) return err } span.SetStatus(codes.Ok, \u0026#34;Operation completed successfully\u0026#34;) return nil } Status Codes OpenTelemetry defines two main status codes:\ncodes.Ok: Indicates that the operation completed successfully. codes.Error: Indicates that the operation encountered an error. Best Practices for Error Handling Always set the span status at the end of an operation. Use RecordError to add error details to the span. Include enough context in the error message to aid in debugging. Consider adding custom attributes to provide more context about the error. span.SetAttributes( attribute.String(\u0026#34;error.type\u0026#34;, \u0026#34;database_connection\u0026#34;), attribute.Int(\u0026#34;retry_attempt\u0026#34;, retryCount), ) HTTP vs gRPC: When to Use Each OpenTelemetry supports both HTTP and gRPC for exporting telemetry data. The choice between them depends on your specific use case and infrastructure.\nHTTP Exporter Use HTTP when:\nYou have firewalls or proxies that are already configured for HTTP traffic. You need to support legacy systems that don\u0026rsquo;t support gRPC. You want a simpler setup process, especially in environments where gRPC might be challenging to configure. Example of setting up an HTTP exporter:\nimport ( \u0026#34;go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracehttp\u0026#34; \u0026#34;go.opentelemetry.io/otel/sdk/trace\u0026#34; ) func initTracer() (*trace.TracerProvider, error) { exporter, err := otlptracehttp.New(context.Background(), otlptracehttp.WithInsecure(), otlptracehttp.WithEndpoint(\u0026#34;collector:4318\u0026#34;), ) if err != nil { return nil, err } tp := trace.NewTracerProvider( trace.WithBatcher(exporter), // ... other options ) return tp, nil } gRPC Exporter Use gRPC when:\nYou need high-performance, low-latency data transmission. You want to take advantage of features like bi-directional streaming. You\u0026rsquo;re working in a microservices architecture where gRPC is already in use. Example of setting up a gRPC exporter:\nimport ( \u0026#34;go.opentelemetry.io/otel/exporters/otlp/otlptrace/otlptracegrpc\u0026#34; \u0026#34;go.opentelemetry.io/otel/sdk/trace\u0026#34; ) func initTracer() (*trace.TracerProvider, error) { exporter, err := otlptracegrpc.New(context.Background(), otlptracegrpc.WithInsecure(), otlptracegrpc.WithEndpoint(\u0026#34;collector:4317\u0026#34;), ) if err != nil { return nil, err } tp := trace.NewTracerProvider( trace.WithBatcher(exporter), // ... other options ) return tp, nil } Context Propagation in Depth Context propagation is a crucial aspect of distributed tracing. It allows you to track a request as it moves through different services and components of your system.\nUnderstanding Context Propagation When a request moves from one service to another, the trace context needs to be propagated to maintain the continuity of the trace. This is typically done by passing certain headers in HTTP requests or metadata in gRPC calls.\nW3C Trace Context OpenTelemetry uses the W3C Trace Context standard by default. This standard defines two headers:\ntraceparent: Contains the trace ID, span ID, and trace flags. tracestate: Allows vendors to add additional information to the trace. Implementing Context Propagation Here\u0026rsquo;s a comprehensive example of how to implement context propagation in a Go application:\nimport ( \u0026#34;context\u0026#34; \u0026#34;net/http\u0026#34; \u0026#34;go.opentelemetry.io/otel\u0026#34; \u0026#34;go.opentelemetry.io/otel/propagation\u0026#34; \u0026#34;go.opentelemetry.io/otel/trace\u0026#34; ) // Set up the global propagator func init() { otel.SetTextMapPropagator(propagation.NewCompositeTextMapPropagator(propagation.TraceContext{}, propagation.Baggage{})) } // Server-side: Extract context from incoming request func serverMiddleware(next http.HandlerFunc) http.HandlerFunc { return func(w http.ResponseWriter, r *http.Request) { ctx := otel.GetTextMapPropagator().Extract(r.Context(), propagation.HeaderCarrier(r.Header)) tracer := otel.Tracer(\u0026#34;server\u0026#34;) ctx, span := tracer.Start(ctx, \u0026#34;server_operation\u0026#34;) defer span.End() // Update the request with the new context r = r.WithContext(ctx) next.ServeHTTP(w, r) } } // Client-side: Inject context into outgoing request func makeClientRequest(ctx context.Context, url string) (*http.Response, error) { req, err := http.NewRequestWithContext(ctx, \u0026#34;GET\u0026#34;, url, nil) if err != nil { return nil, err } // Inject the context into the request headers otel.GetTextMapPropagator().Inject(ctx, propagation.HeaderCarrier(req.Header)) client := \u0026amp;http.Client{} return client.Do(req) } // Example usage func handleRequest(w http.ResponseWriter, r *http.Request) { ctx := r.Context() tracer := otel.Tracer(\u0026#34;my-service\u0026#34;) ctx, span := tracer.Start(ctx, \u0026#34;handle_request\u0026#34;) defer span.End() // Make a call to another service resp, err := makeClientRequest(ctx, \u0026#34;http://other-service/api\u0026#34;) if err != nil { span.RecordError(err) span.SetStatus(codes.Error, err.Error()) http.Error(w, \u0026#34;Internal Server Error\u0026#34;, http.StatusInternalServerError) return } defer resp.Body.Close() // Process the response... span.SetStatus(codes.Ok, \u0026#34;Request processed successfully\u0026#34;) } func main() { http.HandleFunc(\u0026#34;/api\u0026#34;, serverMiddleware(handleRequest)) http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) } In this example:\nWe set up a global propagator that uses both the W3C Trace Context and Baggage. The serverMiddleware function extracts the context from incoming requests. The makeClientRequest function injects the context into outgoing requests. The handleRequest function demonstrates how to use the propagated context to create child spans and make downstream requests. Baggage In addition to trace context, OpenTelemetry also supports Baggage, which allows you to propagate key-value pairs across service boundaries:\nimport \u0026#34;go.opentelemetry.io/otel/baggage\u0026#34; // Adding baggage bag, _ := baggage.New(context.Background()) bag, _ = baggage.NewMember(\u0026#34;user.id\u0026#34;, \u0026#34;12345\u0026#34;) ctx = baggage.ContextWithBaggage(ctx, bag) // Retrieving baggage if member := baggage.FromContext(ctx).Member(\u0026#34;user.id\u0026#34;); member.Key() != \u0026#34;\u0026#34; { userID := member.Value() // Use userID... } Best Practices for Context Propagation Always use the global propagator to ensure consistency across your application. Be mindful of the data you\u0026rsquo;re propagating, especially with Baggage, as it adds overhead to each request. Ensure that all your services and libraries use compatible versions of OpenTelemetry to avoid propagation issues. Consider implementing custom propagators if you need to support legacy systems or have specific requirements. By properly implementing context propagation, you can create comprehensive traces that span multiple services, giving you a complete picture of your system\u0026rsquo;s behavior and performance.\nWhy Tech Giants Adopt OpenTelemetry Several key features make OpenTelemetry attractive to large-scale technology companies:\nVendor Neutrality OpenTelemetry\u0026rsquo;s vendor-neutral approach allows companies to avoid vendor lock-in and freely choose or switch between different observability backends. Standardization By providing a standard for instrumentation and telemetry data, OpenTelemetry simplifies the process of monitoring complex, heterogeneous systems. Extensibility The plugin architecture of OpenTelemetry allows for easy extension and customization to meet specific needs. Performance OpenTelemetry is designed with performance in mind, with features like efficient context propagation and flexible sampling strategies. Comprehensive Coverage Support for traces, metrics, and logs in a single framework simplifies the observability stack. Cloud Native OpenTelemetry integrates well with cloud-native technologies and supports modern deployment patterns like microservices and serverless architectures. Community Support Being an open-source project with broad industry support ensures continuous improvement and long-term viability. Best Practices for OpenTelemetry Implementation Start with Key Services Begin by instrumenting your most critical services to gain immediate value. Use Automatic Instrumentation Leverage automatic instrumentation libraries where possible to quickly add observability with minimal code changes. Enrich with Manual Instrumentation Add manual instrumentation for business-specific operations and custom metrics. Implement Effective Sampling Design your sampling strategy to balance data volume with observability needs. Standardize Naming and Attributes Develop and enforce conventions for span names and attribute keys across your organization. Monitor OpenTelemetry Itself Implement monitoring for your OpenTelemetry components, especially the Collector, to ensure reliable telemetry collection. Educate Your Team Ensure that developers understand OpenTelemetry concepts and best practices for effective use. Conclusion OpenTelemetry represents a significant advancement in the field of observability, offering a standardized, vendor-neutral approach to telemetry data collection and management. Its comprehensive feature set, flexibility, and strong community support make it an attractive choice for organizations of all sizes, from startups to tech giants.\nBy providing a unified framework for traces, metrics, and logs, OpenTelemetry simplifies the implementation of observability in complex, distributed systems. Its adoption can lead to improved system understanding, faster problem resolution, and ultimately, better software quality and user experience.\nAs the project continues to evolve, staying informed about new features and best practices will be crucial for organizations looking to maximize the benefits of their observability efforts. With its growing ecosystem and industry support, OpenTelemetry is well-positioned to become the de facto standard for observability in the coming years.\n","permalink":"http://backendbyte.com/posts/2024-07-09-opentelemetry-deep-dive-2024/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/static/images/posts/2024/openTelemetry/OpenTelemetry.png\" alt=\"OpenTelemetry Demystified: From Concept to Implementation\"  /\u003e\n\u003c/p\u003e\n\u003ch2 id=\"introduction-to-opentelemetry\"\u003eIntroduction to OpenTelemetry\u003c/h2\u003e\n\u003cp\u003eOpenTelemetry is an open-source observability framework designed to create and manage telemetry data such as traces, metrics, and logs. It provides a collection of tools, APIs, and SDKs used to instrument, generate, collect, and export telemetry data for analysis. The project aims to standardize the way telemetry data is collected and transmitted, making it easier for organizations to gain insights into their software\u0026rsquo;s performance and behavior across different environments and technologies.\u003c/p\u003e","title":"OpenTelemetry Demystified: From Concept to Implementation"},{"content":" Go\u0026rsquo;s net/http package is a powerful and versatile library for building HTTP clients and servers. It provides a robust set of tools for handling HTTP requests and responses, making it an essential component for web development in Go. This article will dive deep into the net/http package, exploring its core concepts, key features, and practical applications.\nOverview of the net/http Package: The net/http package is part of Go\u0026rsquo;s standard library and offers a high-level interface for HTTP client and server implementations. It abstracts away many of the complexities involved in network communication, allowing developers to focus on application logic rather than low-level network details.\nKey components of the net/http package include: HTTP client functionality HTTP server functionality Request and response handling URL parsing and manipulation Cookie management File serving HTTPS support HTTP Client: The net/http package provides a default HTTP client that can be used to send HTTP requests to servers. Here\u0026rsquo;s a basic example of how to use the HTTP client:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;io/ioutil\u0026#34; \u0026#34;net/http\u0026#34; ) func main() { resp, err := http.Get(\u0026#34;https://api.example.com/data\u0026#34;) if err != nil { fmt.Println(\u0026#34;Error:\u0026#34;, err) return } defer resp.Body.Close() body, err := ioutil.ReadAll(resp.Body) if err != nil { fmt.Println(\u0026#34;Error reading response:\u0026#34;, err) return } fmt.Println(\u0026#34;Response:\u0026#34;, string(body)) } This example demonstrates a simple GET request to an API endpoint. The http.Get() function returns a response and an error. After checking for errors, we read the response body and print it.\nThe net/http package also supports other HTTP methods like POST, PUT, DELETE, etc. Here\u0026rsquo;s an example of a POST request:\nfunc postExample() { data := []byte(`{\u0026#34;name\u0026#34;: \u0026#34;John Doe\u0026#34;, \u0026#34;age\u0026#34;: 30}`) resp, err := http.Post(\u0026#34;https://api.example.com/users\u0026#34;, \u0026#34;application/json\u0026#34;, bytes.NewBuffer(data)) if err != nil { fmt.Println(\u0026#34;Error:\u0026#34;, err) return } defer resp.Body.Close() fmt.Println(\u0026#34;Status:\u0026#34;, resp.Status) } HTTP Server: Creating an HTTP server with the net/http package is straightforward. Here\u0026rsquo;s a basic example:\npackage main import ( \u0026#34;fmt\u0026#34; \u0026#34;net/http\u0026#34; ) func helloHandler(w http.ResponseWriter, r *http.Request) { fmt.Fprintf(w, \u0026#34;Hello, World!\u0026#34;) } func main() { http.HandleFunc(\u0026#34;/\u0026#34;, helloHandler) fmt.Println(\u0026#34;Server is running on http://localhost:8080\u0026#34;) http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) } This example sets up a simple HTTP server that responds with \u0026ldquo;Hello, World!\u0026rdquo; to all requests. The http.HandleFunc() function registers a handler function for a specific path, and http.ListenAndServe() starts the server on the specified port.\nRequest and Response Handling: The net/http package provides types for handling HTTP requests and responses:\nhttp.Request: Represents an HTTP request received by a server or to be sent by a client. http.ResponseWriter: An interface used by an HTTP handler to construct an HTTP response. Here\u0026rsquo;s an example of a more complex handler that demonstrates request parsing and response writing:\nfunc userHandler(w http.ResponseWriter, r *http.Request) { if r.Method != http.MethodPost { http.Error(w, \u0026#34;Method not allowed\u0026#34;, http.StatusMethodNotAllowed) return } err := r.ParseForm() if err != nil { http.Error(w, \u0026#34;Error parsing form\u0026#34;, http.StatusBadRequest) return } name := r.FormValue(\u0026#34;name\u0026#34;) age := r.FormValue(\u0026#34;age\u0026#34;) w.Header().Set(\u0026#34;Content-Type\u0026#34;, \u0026#34;application/json\u0026#34;) w.WriteHeader(http.StatusCreated) fmt.Fprintf(w, `{\u0026#34;name\u0026#34;: \u0026#34;%s\u0026#34;, \u0026#34;age\u0026#34;: %s}`, name, age) } This handler checks the HTTP method, parses form data, and sends a JSON response with appropriate headers and status code.\nURL Parsing and Manipulation: The net/url package, which is closely related to net/http, provides functionality for URL parsing and manipulation. Here\u0026rsquo;s an example:\nimport ( \u0026#34;fmt\u0026#34; \u0026#34;net/url\u0026#34; ) func urlExample() { u, err := url.Parse(\u0026#34;https://example.com/path?key=value\u0026#34;) if err != nil { fmt.Println(\u0026#34;Error:\u0026#34;, err) return } fmt.Println(\u0026#34;Scheme:\u0026#34;, u.Scheme) fmt.Println(\u0026#34;Host:\u0026#34;, u.Host) fmt.Println(\u0026#34;Path:\u0026#34;, u.Path) fmt.Println(\u0026#34;Query:\u0026#34;, u.Query()) u.Path = \u0026#34;/newpath\u0026#34; u.RawQuery = \u0026#34;newkey=newvalue\u0026#34; fmt.Println(\u0026#34;Modified URL:\u0026#34;, u.String()) } This example demonstrates parsing a URL, accessing its components, and modifying it.\nCookie Management: The net/http package provides support for handling HTTP cookies. Here\u0026rsquo;s an example of setting and reading cookies:\nfunc cookieHandler(w http.ResponseWriter, r *http.Request) { // Setting a cookie cookie := \u0026amp;http.Cookie{ Name: \u0026#34;session_id\u0026#34;, Value: \u0026#34;12345\u0026#34;, Path: \u0026#34;/\u0026#34;, MaxAge: 300, } http.SetCookie(w, cookie) // Reading a cookie sessionCookie, err := r.Cookie(\u0026#34;session_id\u0026#34;) if err != nil { if err == http.ErrNoCookie { fmt.Fprintln(w, \u0026#34;No session cookie found\u0026#34;) } else { http.Error(w, \u0026#34;Error reading cookie\u0026#34;, http.StatusInternalServerError) } return } fmt.Fprintf(w, \u0026#34;Session ID: %s\u0026#34;, sessionCookie.Value) } This handler sets a session cookie and then reads it back from the request.\nFile Serving: The net/http package makes it easy to serve static files. Here\u0026rsquo;s an example:\nfunc main() { fs := http.FileServer(http.Dir(\u0026#34;./static\u0026#34;)) http.Handle(\u0026#34;/static/\u0026#34;, http.StripPrefix(\u0026#34;/static/\u0026#34;, fs)) fmt.Println(\u0026#34;Server is running on http://localhost:8080\u0026#34;) http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) } This code sets up a file server to serve files from the \u0026ldquo;./static\u0026rdquo; directory under the \u0026ldquo;/static/\u0026rdquo; URL path.\nHTTPS Support: The net/http package also supports HTTPS. Here\u0026rsquo;s how to start an HTTPS server:\nfunc main() { http.HandleFunc(\u0026#34;/\u0026#34;, helloHandler) fmt.Println(\u0026#34;HTTPS server is running on https://localhost:8443\u0026#34;) err := http.ListenAndServeTLS(\u0026#34;:8443\u0026#34;, \u0026#34;server.crt\u0026#34;, \u0026#34;server.key\u0026#34;, nil) if err != nil { fmt.Println(\u0026#34;Error:\u0026#34;, err) } } This example starts an HTTPS server using a certificate and private key file.\nMiddleware: Middleware functions can be used to wrap HTTP handlers to add additional functionality. Here\u0026rsquo;s an example of a simple logging middleware:\nfunc loggingMiddleware(next http.HandlerFunc) http.HandlerFunc { return func(w http.ResponseWriter, r *http.Request) { start := time.Now() next.ServeHTTP(w, r) fmt.Printf(\u0026#34;%s %s %v\\n\u0026#34;, r.Method, r.URL.Path, time.Since(start)) } } func main() { http.HandleFunc(\u0026#34;/\u0026#34;, loggingMiddleware(helloHandler)) http.ListenAndServe(\u0026#34;:8080\u0026#34;, nil) } This middleware logs the request method, path, and duration for each request.\nCustom Transport and Client: For more advanced use cases, you can create custom HTTP transports and clients:\nfunc customClientExample() { tr := \u0026amp;http.Transport{ MaxIdleConns: 10, IdleConnTimeout: 30 * time.Second, DisableCompression: true, } client := \u0026amp;http.Client{ Transport: tr, Timeout: time.Second * 10, } resp, err := client.Get(\u0026#34;https://api.example.com/data\u0026#34;) if err != nil { fmt.Println(\u0026#34;Error:\u0026#34;, err) return } defer resp.Body.Close() // Process the response... } This example creates a custom HTTP client with a configured transport, allowing fine-grained control over connection pooling, timeouts, and other settings.\nConclusion: The net/http package in Go provides a powerful and flexible foundation for building HTTP clients and servers. Its simplicity and efficiency make it an excellent choice for web development in Go. This article has covered the core concepts and features of the package, including client and server implementations, request and response handling, URL manipulation, cookie management, file serving, and HTTPS support.\nBy leveraging the net/http package, developers can create robust and scalable web applications with ease. Whether you\u0026rsquo;re building a simple API client or a complex web server, the net/http package offers the tools you need to handle HTTP communication effectively in your Go projects.\n","permalink":"http://backendbyte.com/posts/2024-07-09-db-net-http-package-golang-2024/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/static/images/posts/2024/net-http/net-http-package-golang.png\" alt=\"Understanding Go\u0026rsquo;s net/http Package: A Comprehensive Guide\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003eGo\u0026rsquo;s net/http package is a powerful and versatile library for building HTTP clients and servers. It provides a robust set of tools for handling HTTP requests and responses, making it an essential component for web development in Go. This article will dive deep into the net/http package, exploring its core concepts, key features, and practical applications.\u003c/p\u003e\n\u003ch2 id=\"overview-of-the-nethttp-package\"\u003eOverview of the net/http Package:\u003c/h2\u003e\n\u003cp\u003eThe net/http package is part of Go\u0026rsquo;s standard library and offers a high-level interface for HTTP client and server implementations. It abstracts away many of the complexities involved in network communication, allowing developers to focus on application logic rather than low-level network details.\u003c/p\u003e","title":"Understanding Go's net/http Package: A Comprehensive Guide"},{"content":" When dealing with massive datasets in PostgreSQL, efficiency becomes crucial. Recently, I faced a challenge while inserting 20 million records into a database while working on one of my hobby projects. This experience led me to explore various optimization techniques, from query optimization to server configuration tweaks.\nThe Journey from INSERT to COPY Initially, I used the traditional INSERT approach, which proved to be excruciatingly slow, taking hours to complete. This prompted me to search for a more efficient solution, leading me to the COPY command.\nThe command format is:\nCOPY table_name (column1, column2, column3, ...) FROM \u0026#39;/path/to/your/file.csv\u0026#39; WITH (FORMAT csv, HEADER true); Here\u0026rsquo;s the COPY command I used:\n-- Load data into products_with_index COPY products_with_index (product_name, product_price, product_desc, origin_country, manufacture_date) FROM \u0026#39;/data/products.csv\u0026#39; WITH (FORMAT csv, HEADER true); This command allows PostgreSQL to directly read from a CSV file and insert the data into the specified table. The HEADER true option tells PostgreSQL to skip the first line of the CSV, assuming it contains column names.\nThe performance improvement was significant:\nINSERT method: Several hours COPY command: Minutes Beyond COPY: Fine-tuning PostgreSQL Configuration While COPY dramatically improved insertion speed, I discovered that server configuration plays a crucial role in optimizing large-scale data operations. Two key parameters I adjusted were max_wal_size and checkpoint_timeout.\nUnderstanding and Adjusting max_wal_size The max_wal_size parameter is particularly important for bulk insert operations. WAL stands for Write-Ahead Logging, a standard method for ensuring data integrity. When you insert data, PostgreSQL first writes the changes to the WAL before modifying the actual data files. This ensures that if the system crashes, the database can recover to a consistent state.\nHowever, during large bulk inserts, the default WAL size can become a bottleneck. Here\u0026rsquo;s why:\nDefault Limitation: The default settings of checkpoint_timeout and max_wal_size are 5 minutes and 1 GB, respectively. For massive data insertions, this can lead to frequent checkpoints, crushing down the process.\nCheckpoint Frequency: When the WAL reaches max_wal_size, PostgreSQL triggers a checkpoint, flushing all changes to disk. Frequent checkpoints during bulk inserts can significantly slow down the operation.\nI/O Impact: Each checkpoint involves substantial I/O operations, which can interfere with the ongoing data insertion.\nTo address this, I increased max_wal_size to 5GB:\nmax_wal_size = 5GB This larger WAL size allows more data to be inserted between checkpoints, reducing I/O overhead and speeding up the bulk insert process.\nComplementing with checkpoint_timeout To further optimize the process, I also adjusted the checkpoint_timeout:\ncheckpoint_timeout = 30min This setting ensures that even if the WAL doesn\u0026rsquo;t reach max_wal_size, a checkpoint will still occur every 30 minutes. This provides a balance between performance and data safety.\nImplementing the Changes The postgresql.conf file, I added the new settings:\nmax_wal_size = 5GB checkpoint_timeout = 30min This configuration should be put inside /etc/postgresql/postgresql.conf\nTo apply these configurations, I modified my Docker Compose file to include:\ncommand: - \u0026#34;postgres\u0026#34; - \u0026#34;-c\u0026#34; - \u0026#34;config_file=/etc/postgresql/postgresql.conf\u0026#34; Results and Considerations After implementing both the COPY command and these configuration changes, the insertion of 20 million records went from a hours-long process to completing in just minutes. However, it\u0026rsquo;s important to note:\nData Validation: COPY bypasses some constraints and triggers. Always validate your data before using COPY, especially in production environments.\nResource Usage: Increasing max_wal_size means more disk space is used for the WAL. Ensure your system has sufficient storage.\nRecovery Time: Larger WAL sizes can increase database recovery time in case of a crash. Balance this against your uptime requirements.\nTesting: Always test these changes in a non-production environment first to understand their full impact on your specific workload.\nConclusion Optimizing PostgreSQL for large-scale data insertions involves both query-level optimizations (like using COPY instead of INSERT) and server-level configurations. By understanding and adjusting parameters like max_wal_size, we can significantly improve the performance of bulk data operations.\nRemember, while these optimizations can provide substantial performance benefits, they should be applied thoughtfully, with consideration for your specific use case, hardware resources, and data integrity requirements.\nFor further reference, you can read this: https://www.postgresql.org/docs/current/populate.html\n","permalink":"http://backendbyte.com/posts/2024-07-08-postgres-bulk-insert-2024/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/static/images/posts/2024/postgres-copy-cmd/postgresql-copy-cmd.png\" alt=\"Optimizing PostgreSQL for Large-Scale Data Insertion\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003eWhen dealing with massive datasets in PostgreSQL, efficiency becomes crucial. Recently, I faced a challenge while inserting 20 million records into a database while working on one of my hobby projects. This experience led me to explore various optimization techniques, from query optimization to server configuration tweaks.\u003c/p\u003e\n\u003ch2 id=\"the-journey-from-insert-to-copy\"\u003eThe Journey from INSERT to COPY\u003c/h2\u003e\n\u003cp\u003eInitially, I used the traditional INSERT approach, which proved to be excruciatingly slow, taking hours to complete. This prompted me to search for a more efficient solution, leading me to the COPY command.\u003c/p\u003e","title":"Optimizing PostgreSQL for Large-Scale Data Insertions: From INSERT to COPY and Beyond"},{"content":" In the realm of containerized applications, particularly those orchestrated with Docker, developers often encounter a subtle yet critical challenge: managing the startup sequence of interdependent services. One of the most common and potentially problematic scenarios is the race condition that can occur between a database container and an application container. This article will provide an in-depth exploration of this race condition, with a focus on Go-based applications, and offer strategies to mitigate it effectively.\nUnderstanding Race Conditions A race condition, in its essence, is a situation where the behavior of a system depends on the relative timing of events, particularly when those events don\u0026rsquo;t occur in the intended or expected order. In concurrent systems, race conditions often manifest when two or more operations must execute in a specific sequence to function correctly, but the system doesn\u0026rsquo;t guarantee this order.\nThe Database-Application Race Condition in Docker: A Deeper Look When deploying a multi-container Docker application, typically comprising a database (such as PostgreSQL or MySQL) and an application server (in our case, a Go application), you might encounter a scenario where your application fails to start due to an inability to connect to the database. This failure is a classic example of a race condition in containerized environments.\nTo understand why this occurs, let\u0026rsquo;s break down the startup process:\nConcurrent Initialization: When you start your Docker Compose setup or deploy your containers, Docker initiates the startup of all defined services concurrently. This parallel initialization is generally beneficial for reducing overall startup time.\nVarying Startup Times: Different containers have different initialization requirements. A database container, for instance, needs to perform several operations before it\u0026rsquo;s ready to accept connections:\nInitialize the database engine Load configuration files Allocate memory and resources Create or recover database files Start listening for connections On the other hand, an application container might only need to load its code into memory and start the server process.\nEager Application Initialization: Most application code is written with the assumption that all required services (like databases) are available when the application starts. As soon as the application container starts, it typically attempts to establish a connection to the database.\nTiming Mismatch: Due to the difference in startup times, the application container often becomes operational before the database container is ready to accept connections. When the application attempts to connect to the database, it encounters an error because the database service isn\u0026rsquo;t available yet.\nThis mismatch in timing - where the application is ready before its critical dependency (the database) - is the essence of the race condition we\u0026rsquo;re discussing.\nExample Scenario with Go Let\u0026rsquo;s illustrate this with a Go application that connects to a PostgreSQL database.\ndocker-compose.yml:\nversion: \u0026#39;3\u0026#39; services: db: image: postgres:13 environment: POSTGRES_DB: myapp POSTGRES_USER: user POSTGRES_PASSWORD: password app: build: . depends_on: - db environment: DATABASE_URL: postgres://user:password@db:5432/myapp?sslmode=disable main.go:\npackage main import ( \u0026#34;database/sql\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;time\u0026#34; _ \u0026#34;github.com/lib/pq\u0026#34; ) func main() { dbURL := os.Getenv(\u0026#34;DATABASE_URL\u0026#34;) db, err := sql.Open(\u0026#34;postgres\u0026#34;, dbURL) if err != nil { fmt.Println(\u0026#34;Error opening database connection:\u0026#34;, err) os.Exit(1) } defer db.Close() err = db.Ping() if err != nil { fmt.Println(\u0026#34;Error connecting to the database:\u0026#34;, err) os.Exit(1) } fmt.Println(\u0026#34;Successfully connected to the database\u0026#34;) } In this scenario, even though we\u0026rsquo;ve used depends_on in the Docker Compose file, our Go application might still attempt to connect before the PostgreSQL database is ready to accept connections. This will result in a connection error, exemplifying the race condition.\nStrategies to Mitigate the Race Condition 1. Implement Retry Logic One robust approach is to implement retry logic in your Go application. This method allows your application to gracefully handle initial connection failures and retry until the database becomes available.\nUpdated main.go:\npackage main import ( \u0026#34;database/sql\u0026#34; \u0026#34;fmt\u0026#34; \u0026#34;os\u0026#34; \u0026#34;time\u0026#34; _ \u0026#34;github.com/lib/pq\u0026#34; ) func main() { dbURL := os.Getenv(\u0026#34;DATABASE_URL\u0026#34;) db, err := connectWithRetry(dbURL) if err != nil { fmt.Println(\u0026#34;Failed to connect to database:\u0026#34;, err) os.Exit(1) } defer db.Close() fmt.Println(\u0026#34;Successfully connected to the database\u0026#34;) } func connectWithRetry(dbURL string) (*sql.DB, error) { var db *sql.DB var err error maxRetries := 5 retryDelay := time.Second * 5 for i := 0; i \u0026lt; maxRetries; i++ { db, err = sql.Open(\u0026#34;postgres\u0026#34;, dbURL) if err != nil { fmt.Printf(\u0026#34;Error opening database connection (attempt %d/%d): %v\\n\u0026#34;, i+1, maxRetries, err) time.Sleep(retryDelay) continue } err = db.Ping() if err == nil { return db, nil } fmt.Printf(\u0026#34;Error connecting to the database (attempt %d/%d): %v\\n\u0026#34;, i+1, maxRetries, err) time.Sleep(retryDelay) } return nil, fmt.Errorf(\u0026#34;failed to connect to the database after %d attempts\u0026#34;, maxRetries) } 2. Use Docker Healthchecks Docker provides a HEALTHCHECK instruction that can be used to inform Docker how to test a container to check its health status. This can be particularly useful in ensuring that dependent services only start when their prerequisites are fully operational.\nUpdate your docker-compose.yml:\nversion: \u0026#39;3\u0026#39; services: db: image: postgres:13 environment: POSTGRES_DB: myapp POSTGRES_USER: user POSTGRES_PASSWORD: password healthcheck: test: [\u0026#34;CMD-SHELL\u0026#34;, \u0026#34;pg_isready -U user -d myapp\u0026#34;] interval: 10s timeout: 5s retries: 5 app: build: . depends_on: db: condition: service_healthy environment: DATABASE_URL: postgres://user:password@db:5432/myapp?sslmode=disable This configuration ensures that the app service only starts after the db service is healthy and ready to accept connections.\n3. Use a Startup Script Another approach is to use a startup script that checks for database availability before starting your Go application.\nCreate a start.sh script:\n#!/bin/sh set -e until PGPASSWORD=$POSTGRES_PASSWORD psql -h \u0026#34;db\u0026#34; -U \u0026#34;user\u0026#34; -d \u0026#34;myapp\u0026#34; -c \u0026#39;\\q\u0026#39;; do \u0026gt;\u0026amp;2 echo \u0026#34;Postgres is unavailable - sleeping\u0026#34; sleep 1 done \u0026gt;\u0026amp;2 echo \u0026#34;Postgres is up - executing command\u0026#34; exec \u0026#34;$@\u0026#34; Update your Dockerfile:\nFROM golang:1.16 WORKDIR /app COPY go.mod go.sum ./ RUN go mod download COPY . . RUN go build -o main . COPY start.sh /start.sh RUN chmod +x /start.sh CMD [\u0026#34;/start.sh\u0026#34;, \u0026#34;./main\u0026#34;] Conclusion Race conditions between databases and applications in Docker environments represent a significant challenge in ensuring smooth and reliable application startup. The strategies presented here - retry logic, Docker healthchecks, and startup scripts - each offer unique advantages and can be applied based on your specific requirements.\nBy implementing these solutions, particularly the retry logic in your Go application, you create a more robust and fault-tolerant system. This approach not only addresses the immediate issue of the race condition but also makes your application more resilient to temporary network issues or database restarts during normal operation.\nRemember, in production environments, it\u0026rsquo;s often beneficial to implement a combination of these strategies. This multi-layered approach ensures the highest level of reliability and provides multiple safeguards against potential startup issues.\nUnderstanding and effectively managing these race conditions is crucial in developing stable, dependable containerized applications. By doing so, you enhance both the developer experience and the overall reliability of your application infrastructure.\n","permalink":"http://backendbyte.com/posts/2024-07-08-db-race-condition-notes-2024/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/static/images/posts/2024/race-condition/race-condition.png\" alt=\"Race Condition between Database and Application in Docker Container\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003eIn the realm of containerized applications, particularly those orchestrated with Docker, developers often encounter a subtle yet critical challenge: managing the startup sequence of interdependent services. One of the most common and potentially problematic scenarios is the race condition that can occur between a database container and an application container. This article will provide an in-depth exploration of this race condition, with a focus on Go-based applications, and offer strategies to mitigate it effectively.\u003c/p\u003e","title":"Race Condition between Database and Application in Docker Container"},{"content":"Before starting, lets visualize a illustration how it works User initiates a request to a web application hosted on a cluster server. DNS resolution directs the user to Cloudflare. Cloudflare acts as a reverse proxy, terminating the SSL/TLS connection and initiating a tunnel to the Nginx ingress controller in the Kubernetes cluster. The Nginx ingress controller routes the request based on the Host header to the appropriate service within the cluster. The service distributes traffic across pods running the application. A selected pod processes the request and generates a response. The response travels back through the Nginx ingress controller, Cloudflare tunnel, and finally to the user\u0026rsquo;s device.\nInstall and configure argocd and nginx-ingress First we have install argocd high availability version on our bare metal server. To install this,\nsudo kubectl create namespace argocd sudo kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/v2.10.1/manifests/ha/install.yaml Download With Curl (argocd CLI) wget https://github.com/argoproj/argo-cd/releases/download/v2.10.1/argocd-linux-amd64 mv argocd-linux-amd64 argocd chmod +x argocd mv argocd /usr/local/bin/ Retrieve this password using the argocd CLI: sudo kubectl -n argocd get secret sudo kubectl -n argocd get secrets argocd-initial-admin-secret -o json sudo kubectl -n argocd get secrets argocd-initial-admin-secret -o json | jq .data.password -r | base64 -d To edit any service of argocd sudo kubectl -n argocd edit svc argocd-server Create argocd-ingress rule to route trafic ## argocd-ingress.yml apiVersion: networking.k8s.io/v1 kind: Ingress metadata: name: argocd-ingress namespace: argocd annotations: kubernetes.io/ingress.class: \u0026#34;nginx\u0026#34; alb.ingress.kubernetes.io/ssl-passthrough: \u0026#34;true\u0026#34; nginx.ingress.kubernetes.io/force-ssl-redirect: \u0026#34;false\u0026#34; nginx.ingress.kubernetes.io/backend-protocol: \u0026#34;HTTP\u0026#34; spec: rules: - host: argocd.example.com http: paths: - pathType: Prefix path: / backend: service: name: argocd-server port: number: 80 Create argocd-tunnel for configure cloudflare ## argocd-tunnel.yml apiVersion: apps/v1 kind: Deployment metadata: namespace: argocd labels: app: cloudflared-argocd name: cloudflared-argocd spec: replicas: 1 selector: matchLabels: app: cloudflared-argocd template: metadata: labels: app: cloudflared-argocd spec: containers: - name: cloudflared-argocd image: cloudflare/cloudflared:latest # image: ghcr.io/maggie0002/cloudflared:2022.7.1 imagePullPolicy: Always args: [ \u0026#34;tunnel\u0026#34;, \u0026#34;--no-autoupdate\u0026#34;, \u0026#34;run\u0026#34;, \u0026#34;--token=place_your_token_here\u0026#34;, ] restartPolicy: Always terminationGracePeriodSeconds: 60 Configure nginx-ingress for handle ingress trafic Link: https://kubernetes.github.io/ingress-nginx/deploy/\nkubectl apply -f https://raw.githubusercontent.com/kubernetes/ingress-nginx/controller-v1.8.2/deploy/static/provider/baremetal/deploy.yaml Modify the simply set server.insecure: \u0026quot;true\u0026quot; in the argocd-cmd-params-cm ConfigMap\nsudo kubectl -n argocd edit configmap argocd-cmd-params-cm Make sure to stop redirection http to https inside cloudflare\nNext make the change in nginx ingress controller deployment to add the enable-ssl-passthrough flag as shown below\nkubectl edit deploy ingress-nginx-controller -n ingress-nginx Now its time to apply those two argocd-ingress.yml and argocd-tunnel.yml file\nsudo kubectl apply -f argocd-ingress.yml sudo kubectl apply -f argocd-tunnel.yml To make everything work perfectly, roleout the core-dns from kube-system namespace and restart deployment from argocd namespace. kubectl rollout restart deployment coredns -n kube-system sudo kubectl rollout restart deployment --namespace=argocd sudo kubectl rollout restart deployment --namespace=ingress-nginx Operating procedure Create Application before creating application, you must login to the sysyem. first check the argocd-server svc ip, then\nkubectl get svc -n argocd Look for argocd-server ip Login to argocd argocd login 10.43.168.110 argocd app list Create application (using cli) argocd app create auth-api \\ --repo https://net.osl.team:20613/m2saas/core/M2S.AuthAPI.git \\ --path k8s/dev \\ --dest-namespace default \\ --dest-server https://kubernetes.default.svc \\ --directory-recurse \\ --sync-policy automated sync application argocd app sync auth-api Check app logs argocd app logs auth-service Rollback with Argo CD CLI: To rollback to a previous application revision, you can use the argocd app rollback command. You need to specify the name of the application and the target revision you want to roll back to.\nargocd app rollback \u0026lt;APP_NAME\u0026gt; --revision \u0026lt;REVISION_NUMBER\u0026gt; \u0026lt;APP_NAME\u0026gt; is the name of the application you want to rollback. \u0026lt;REVISION_NUMBER\u0026gt; is the target revision number you want to roll back to. You can obtain revision numbers using argocd app history \u0026lt;APP_NAME\u0026gt;. For example:\nargocd app rollback auth-api --revision 3 Rollout with Argo CD CLI: To trigger a manual rollout of an application in Argo CD, you can use the argocd app sync command. This command synchronizes the application state with the desired state defined in the Git repository.\nargocd app sync \u0026lt;APP_NAME\u0026gt; \u0026lt;APP_NAME\u0026gt; is the name of the application you want to trigger a rollout for. For example:\nargocd app sync auth-api if needed, kubectl rollout restart deployment coredns -n kube-system sudo kubectl rollout restart deployment --namespace=argocd For your information, to delete CDR and argocd app- Delete CRDs sudo kubectl patch crd applications.argoproj.io -p \u0026#39;{\u0026#34;metadata\u0026#34;: {\u0026#34;finalizers\u0026#34;: null}}\u0026#39; --type merge sudo kubectl delete crd applications.argoproj.io Delete argocd apps sudo kubectl patch app auth-api-test -p \u0026#39;{\u0026#34;metadata\u0026#34;: {\u0026#34;finalizers\u0026#34;: null}}\u0026#39; --type merge sudo kubectl delete app auth-api Thanks for your read.\nHappy Coding.\n","permalink":"http://backendbyte.com/posts/2024-05-18-argocd-deployment-notes-2024/","summary":"\u003ch2 id=\"before-starting-lets-visualize-a-illustration-how-it-works\"\u003eBefore starting, lets visualize a illustration how it works\u003c/h2\u003e\n\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/static/images/posts/2024/argocd/nginx-ingress.png\" alt=\"nginx-ingress.png\"  /\u003e\n\u003c/p\u003e\n\u003cp\u003eUser initiates a request to a web application hosted on a cluster server. DNS resolution directs the user to Cloudflare. Cloudflare acts as a reverse proxy, terminating the SSL/TLS connection and initiating a tunnel to the Nginx ingress controller in the Kubernetes cluster. The Nginx ingress controller routes the request based on the Host header to the appropriate service within the cluster. The service distributes traffic across pods running the application. A selected pod processes the request and generates a response. The response travels back through the Nginx ingress controller, Cloudflare tunnel, and finally to the user\u0026rsquo;s device.\u003c/p\u003e","title":"ArgoCD configuration on Bare metal with nginx-ingress and cloudflare tunnel"},{"content":"Nearly three years ago, I upgraded to Ubuntu 22.04 LTS. After the upgrade, I ran into problems with pgAdmin 4 when working with PostgreSQL. As a result, I had to rely on the psql interactive shell for all my tasks. Here, I will share some basic psql commands that I frequently use.\n1. Enter psql shell To interact with PostgreSQL from the terminal, enter the psql shell. To enter the psql shell, type:\nsudo su - postgres sudo su - postgres is a command used to run a specific command as the postgres user. After entering this command, type psql to enter the PostgreSQL interactive terminal as the postgres user. It allows you to execute various SQL commands and interact with the PostgreSQL database.\nYou can do the same task with a single command:\nsudo -u postgres psql It will give you access to the PostgreSQL interactive terminal.\n2. Create User To create a user, type:\nCREATE USER username WITH PASSWORD \u0026#39;password\u0026#39;; One thing to mention is to write your password inside the inverted comma. And don’t forget to add a semicolon at the end of the statement.\n3. Create Database To create a database, enter:\nCREATE DATABASE database_name; 4. Add User to Database To add your newly created user to any of your previously created databases, type:\nGRANT ALL PRIVILEGES ON DATABASE \u0026lt;database name\u0026gt; TO \u0026lt;username\u0026gt;; 5. Display Database List To show the created database list, type:\n\\l It will show a list of all databases on the server.\n6. Select Database To switch/connect a specific database, in this case, your newly created one, type:\n\\c \u0026lt;database name\u0026gt; 7. Show Table To show all the tables in your database, type:\n\\dt or\n/d It will print a list of all tables in the current database. The \\dt command lists all the tables in the current database. It is a shortcut for the \\d tables command, which displays only the tables in the database.\n8. Create a Table To create a table, write your queries. We want to create a table LinkedInPost containing id, post_title, post_content, post_date fields. To create this, type this command in your shell:\nCREATE TABLE LinkedInPosts ( id SERIAL PRIMARY KEY, post_title VARCHAR(255), post_content TEXT, post_date TIMESTAMP DEFAULT NOW() ); 9. Rename a Column To rename any column in a table in PostgreSQL using psql interactive shell, you can use the ALTER TABLE statement with the RENAME COLUMN clause. For example, to rename post_title column to post_featured_title, type:\nALTER TABLE LinkedInPost RENAME COLUMN post_title TO post_featured_title; It will rename the column post_title to post_featured_title.\n10. Delete a Column To delete a column from a table in PostgreSQL using psql interactive shell, use the ALTER TABLE statement with the DROP COLUMN clause. For example, to delete the post_content field from your LinkedInPost table, type:\nALTER TABLE LinkedInPost DROP COLUMN post_content; 11. Index a Column To index a column to make your query faster, suppose you want to index the id column. To create an index on the id field of the LinkedInPost table in PostgreSQL, you can use the CREATE INDEX statement. Type:\nCREATE INDEX idx_id ON LinkedInPost(id); It will create an index column idx_id based on the id column.\n12. Delete the Database To delete a table, type:\nDROP DATABASE LinkedInPost; The database you created before is deleted successfully.\n13. Delete Database Issue If you encounter an error when trying to delete the database, like:\nERROR: database \u0026#34;LinkedInPost_db\u0026#34; is being accessed by other user DETAIL: There are 3 other sessions using the database. It occurs because after creating the database, I connect it with pgAdmin 4. If you encounter this type of issue, type:\nSELECT pg_terminate_backend(pg_stat_activity.pid) FROM pg_stat_activity WHERE pg_stat_activity.datname = \u0026#39;LinkedInPost_db\u0026#39; AND pid \u0026lt;\u0026gt; pg_backend_pid(); It will terminate all the connections to the database except the one you are currently using.\nNote: Before running this statement, please log in as the postgres user in the psql shell.\n14. Backup your Database To backup a database, please select the database by typing:\n\\c your_database_name In our case, it will be:\n\\c LinkedInPost Then type:\npg_dump -U postgres -Fc your_database_name \u0026gt; /path/to/your_backup_file_name.dump In this command, -U specifies the user, -Fc specifies the format of the backup file as custom, your_database_name is the name of the database you want to backup, and /path/to/your_backup_file_name.dump is the path and name of the backup file you want to create.\n15. Use the Backup File To import a PostgreSQL backup file with a .dump file extension, you can use the pg_restore command-line tool. Type:\npg_restore -U username -d databasename filename.dump In our case, it will be:\npg_restore -U postgres -d mydatabase mybackupfile.dump You cannot use any custom file type in the pg_dump and pg_restore commands for the backup file type. The pg_dump command creates a custom binary file format for PostgreSQL with the file extension .dump. Similarly, pg_restore is designed to work only with the customized binary format generated by pg_dump.\nWith these commands, you cannot use JSON, Excel, or any other file format. The PostgreSQL custom binary format is widely used in the industry and is considered the standard for PostgreSQL backups and restores. While other file formats are available, such as plain text, CSV, and SQL, the custom binary format is preferred because it allows for faster backup and restore times, supports more advanced features, and is more secure.\nTo exit from the psql terminal, use:\n\\q Configure PostgreSQL with Django If you are completely new to PostgreSQL and trying to connect it with the Python Django Framework, you might face an error during installing psycopg2, a popular PostgreSQL database adapter. You can try to run this code in your terminal:\nsudo apt-get install --reinstall libpq-dev pip install psycopg2 Another problem you might face during python3 manage.py makemigrations and python3 manage.py migrate is:\npsycopg2.errors.InsufficientPrivilege: permission denied for table django_migrations It\u0026rsquo;s because of insufficient permission for the user you created before. To solve this issue, run another script in your psql interactive shell:\n\\c your_database_name GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO \u0026lt;username\u0026gt;; It will solve your issue. You might ask, \u0026ldquo;I already granted permission to the database, why should I run another script again?\u0026rdquo;. The answer is:\n-- Permission on One Table GRANT ALL PRIVILEGES ON TABLE side_adzone TO \u0026lt;username\u0026gt;; -- Permission on All Tables of schema GRANT ALL PRIVILEGES ON ALL TABLES IN SCHEMA public TO \u0026lt;username\u0026gt;; Hope your problem will be solved immediately. I found this piece of code after some trial and error.\nConclusion Learning essential psql command-line skills is an important step toward mastering PostgreSQL as a beginner. With the skills covered in this article, you are now equipped to create and manage databases, tables, and columns, as well as perform basic data manipulation tasks in PostgreSQL. As you continue to practice and learn more about PostgreSQL, you will discover that there is so much more you can do with the psql command-line interface. So go ahead and dive deeper into the world of PostgreSQL, and let the journey of mastering this powerful open-source relational database begin!\nThanks for reading! Hope you found this helpful.\n","permalink":"http://backendbyte.com/posts/2024-05-18-psql-commands-notes-2024/","summary":"\u003cp\u003eNearly three years ago, I upgraded to Ubuntu 22.04 LTS. After the upgrade, I ran into problems with pgAdmin 4 when working with PostgreSQL. As a result, I had to rely on the psql interactive shell for all my tasks. Here, I will share some basic psql commands that I frequently use.\u003c/p\u003e\n\u003ch2 id=\"1-enter-psql-shell\"\u003e1. Enter psql shell\u003c/h2\u003e\n\u003cp\u003eTo interact with PostgreSQL from the terminal, enter the \u003ccode\u003epsql\u003c/code\u003e shell. To enter the \u003ccode\u003epsql\u003c/code\u003e shell, type:\u003c/p\u003e","title":"Getting Started with PostgreSQL:- Essential psql Command Line Skills for Beginners"},{"content":"Introduction This article provides a step-by-step guide for setting up a Kubernetes cluster using k3s on a bare metal server. k3s is a lightweight Kubernetes distribution designed for resource-constrained environments, making it ideal for bare metal setups.\nInstallation Steps 1. Install k3s Run the following command to install k3s on your bare metal server:\ncurl -sfL \u0026lt;https://get.k3s.io\u0026gt; | sh - This command fetches the k3s installation script from the internet and executes it on your server.\n2. Verify Installation After the installation completes, a kubeconfig file will be generated at /etc/rancher/k3s/k3s.yaml, and the kubectl command-line tool provided by k3s will automatically configure itself to use this file.\nTo verify that the installation was successful and that the server is functioning as a Kubernetes master node, run the following command:\nsudo kubectl get nodes If the command returns a list of nodes including the master node, the installation was successful.\nNote: If you encounter permission issues with the kubectl command, try running it as a superuser using sudo -i or sudo su. 3. Access Tokens During the installation process, k3s generates access tokens that can be used to join additional nodes to the cluster. These tokens are stored on the master node.\nTo obtain the access tokens, run the following command:\ncat /var/lib/rancher/k3s/server/node-token Make note of the token value as it will be required when joining worker nodes to the cluster.\nRegister worker node curl -sfL https://get.k3s.io | K3S_URL=https://\u0026lt;server_ip\u0026gt;:6443 K3S_TOKEN=\u0026lt;node_token\u0026gt; sh - Conclusion You have successfully installed k3s on your bare metal server and configured it as a Kubernetes master node. You can now proceed to add worker nodes to your cluster using the generated access tokens.\nFor more information on managing and configuring your k3s cluster, refer to the official k3s documentation:\nhttps://docs.k3s.io/quick-start\nHappy Kubernetes clustering!\n","permalink":"http://backendbyte.com/posts/2024-05-18-k8s-setup-notes-2024/","summary":"\u003ch2 id=\"introduction\"\u003eIntroduction\u003c/h2\u003e\n\u003cp\u003eThis article provides a step-by-step guide for setting up a Kubernetes cluster using k3s on a bare metal server. k3s is a lightweight Kubernetes distribution designed for resource-constrained environments, making it ideal for bare metal setups.\u003c/p\u003e\n\u003ch2 id=\"installation-steps\"\u003eInstallation Steps\u003c/h2\u003e\n\u003ch3 id=\"1-install-k3s\"\u003e1. Install k3s\u003c/h3\u003e\n\u003cp\u003eRun the following command to install k3s on your bare metal server:\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cpre tabindex=\"0\" style=\"color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;\"\u003e\u003ccode class=\"language-bash\" data-lang=\"bash\"\u003e\u003cspan style=\"display:flex;\"\u003e\u003cspan\u003ecurl -sfL \u0026lt;https://get.k3s.io\u0026gt; | sh -\n\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/div\u003e\u003cp\u003eThis command fetches the k3s installation script from the internet and executes it on your server.\u003c/p\u003e","title":"Setting Up a Kubernetes Cluster with k3s on Bare Metal Server"},{"content":" As a engineer at Poridhi, I frequently need to use Git commands to manage my codebase, track changes, collaborate with team members, and maintain an organized workflow. Over the past five months of working in this industry, I\u0026rsquo;ve become familiar with several essential Git commands that have been invaluable in my daily work.\nIn this blog post, I\u0026rsquo;ll share these essential Git commands and provide a brief explanation along with a small example for each one. Whether you\u0026rsquo;re a beginner or an experienced developer, mastering these commands can greatly enhance your productivity and efficiency when working with Git repositories.\ngit status Shows the current state of the working directory, including modified files, staged changes, and untracked files. Example: git status git add Stages changes for the next commit. Example: git add app.go (stages a specific file) or git add . (stages all changes in the current directory) git commit Records staged changes to the repository with a commit message. Example: git commit -m \u0026quot;Add new feature\u0026quot; git push Pushes committed changes to a remote repository. Example: git push origin main (pushes changes to the main branch on the origin remote) git pull Fetches changes from a remote repository and merges them into the current branch. Example: git pull origin main git branch Lists existing branches or creates a new branch. Example: git branch new-feature (creates a new branch named new-feature) git checkout Switches between branches or restores files from earlier commits. Example: git checkout -b features/features-name (switches to the features/features-name branch) git merge Merges changes from one branch into another. Example: git merge new-feature (merges the new-feature branch into the current branch) git rebase Integrates changes from one branch into another by rewriting the commit history. Example: git rebase main (rebases the current branch onto the main branch) git log Shows the commit history. Example: git log -5 (displays the last 5 commits) Example: git log -5 --oneline (displays the last 5 commits in a condensed, one-line-per-commit format, showing the commit hash and message) git clean Removes untracked files and directories from the working directory. Example: git clean -fd (removes untracked files and directories, using the f force and d directories flags) git stash Temporarily shelves changes in the working directory. Example: git stash (stashes current changes) git stash pop Reapplies the most recently stashed changes to the working directory. Example: git stash pop (applies the most recent stash and removes it from the stash list) git cherry-pick Applies specific commits from one branch to another. Example: git cherry-pick \u0026lt;commit-hash\u0026gt; (applies the commit with the specified hash to the current branch) git branch --delete Deletes a specified branch from the local repository. Example: git branch --delete feature/old-feature (deletes the feature/old-feature branch) git reset Resets the working directory and staging area to a specific commit. Example: git reset --hard HEAD (discards all changes since the last commit) These commands cover various aspects of Git workflow, from staging and committing changes to managing branches, integrating changes, and reverting to previous states. By mastering these commands, you\u0026rsquo;ll be able to work more efficiently and collaborate more effectively with your team.\nRemember, Git is a powerful tool, and it\u0026rsquo;s essential to use it responsibly and follow best practices to avoid potential issues or conflicts.\n","permalink":"http://backendbyte.com/posts/2024-05-17-git-notes-2024/","summary":"\u003cp\u003e\u003cimg loading=\"lazy\" src=\"/static/images/posts/default/post-bg-desk.jpg\" alt=\"Header Image\"  /\u003e\n\nAs a engineer at Poridhi, I frequently need to use Git commands to manage my codebase, track changes, collaborate with team members, and maintain an organized workflow. Over the past five months of working in this industry, I\u0026rsquo;ve become familiar with several essential Git commands that have been invaluable in my daily work.\u003c/p\u003e\n\u003cp\u003eIn this blog post, I\u0026rsquo;ll share these essential Git commands and provide a brief explanation along with a small example for each one. Whether you\u0026rsquo;re a beginner or an experienced developer, mastering these commands can greatly enhance your productivity and efficiency when working with Git repositories.\u003c/p\u003e","title":"Essential Git Commands for Daily Use"}]